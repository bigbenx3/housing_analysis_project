{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# King County Real Estate - Housing Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "King County Real Estate has hired us to investigate which features of a home have the greatest effect on price.\n",
    "\n",
    "* They would like us to make a model to predict housing prices.\n",
    "* From that model, they would like to know which factors have the largest effect on price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importing & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset \"kc_house_data.csv\" was obtained from the link below. King County 2014-2015 House Sales dataset\n",
    "\n",
    "https://osf.io/twq9p/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11,8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/bigbenx3/housing_analysis_project/main/kc_house_data.csv\"\n",
    "df1 = pd.read_csv(url, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
       "0      5650     1.0           0     0  ...      7        1180              0   \n",
       "1      7242     2.0           0     0  ...      7        2170            400   \n",
       "2     10000     1.0           0     0  ...      6         770              0   \n",
       "3      5000     1.0           0     0  ...      7        1050            910   \n",
       "4      8080     1.0           0     0  ...      8        1680              0   \n",
       "\n",
       "   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "0      1955             0    98178  47.5112 -122.257           1340   \n",
       "1      1951          1991    98125  47.7210 -122.319           1690   \n",
       "2      1933             0    98028  47.7379 -122.233           2720   \n",
       "3      1965             0    98136  47.5208 -122.393           1360   \n",
       "4      1987             0    98074  47.6168 -122.045           1800   \n",
       "\n",
       "   sqft_lot15  \n",
       "0        5650  \n",
       "1        7639  \n",
       "2        8062  \n",
       "3        5000  \n",
       "4        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sneak preview of all the features in dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Null values present?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21613 entries, 0 to 21612\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   id             21613 non-null  int64  \n",
      " 1   date           21613 non-null  object \n",
      " 2   price          21613 non-null  float64\n",
      " 3   bedrooms       21613 non-null  int64  \n",
      " 4   bathrooms      21613 non-null  float64\n",
      " 5   sqft_living    21613 non-null  int64  \n",
      " 6   sqft_lot       21613 non-null  int64  \n",
      " 7   floors         21613 non-null  float64\n",
      " 8   waterfront     21613 non-null  int64  \n",
      " 9   view           21613 non-null  int64  \n",
      " 10  condition      21613 non-null  int64  \n",
      " 11  grade          21613 non-null  int64  \n",
      " 12  sqft_above     21613 non-null  int64  \n",
      " 13  sqft_basement  21613 non-null  int64  \n",
      " 14  yr_built       21613 non-null  int64  \n",
      " 15  yr_renovated   21613 non-null  int64  \n",
      " 16  zipcode        21613 non-null  int64  \n",
      " 17  lat            21613 non-null  float64\n",
      " 18  long           21613 non-null  float64\n",
      " 19  sqft_living15  21613 non-null  int64  \n",
      " 20  sqft_lot15     21613 non-null  int64  \n",
      "dtypes: float64(5), int64(15), object(1)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21613 non null entries for each columns. Also, all the data, with the exception of date, are numeric (floats and integers). So, that's good. Most likely, \"date\" will be dropped as it's pretty irrelevant as a feature relating to house price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "date             0\n",
       "price            0\n",
       "bedrooms         0\n",
       "bathrooms        0\n",
       "sqft_living      0\n",
       "sqft_lot         0\n",
       "floors           0\n",
       "waterfront       0\n",
       "view             0\n",
       "condition        0\n",
       "grade            0\n",
       "sqft_above       0\n",
       "sqft_basement    0\n",
       "yr_built         0\n",
       "yr_renovated     0\n",
       "zipcode          0\n",
       "lat              0\n",
       "long             0\n",
       "sqft_living15    0\n",
       "sqft_lot15       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still zero null values in all the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minor Data - Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again look back at the sort of data that we have, using the .info() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21613 entries, 0 to 21612\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   id             21613 non-null  int64  \n",
      " 1   date           21613 non-null  object \n",
      " 2   price          21613 non-null  float64\n",
      " 3   bedrooms       21613 non-null  int64  \n",
      " 4   bathrooms      21613 non-null  float64\n",
      " 5   sqft_living    21613 non-null  int64  \n",
      " 6   sqft_lot       21613 non-null  int64  \n",
      " 7   floors         21613 non-null  float64\n",
      " 8   waterfront     21613 non-null  int64  \n",
      " 9   view           21613 non-null  int64  \n",
      " 10  condition      21613 non-null  int64  \n",
      " 11  grade          21613 non-null  int64  \n",
      " 12  sqft_above     21613 non-null  int64  \n",
      " 13  sqft_basement  21613 non-null  int64  \n",
      " 14  yr_built       21613 non-null  int64  \n",
      " 15  yr_renovated   21613 non-null  int64  \n",
      " 16  zipcode        21613 non-null  int64  \n",
      " 17  lat            21613 non-null  float64\n",
      " 18  long           21613 non-null  float64\n",
      " 19  sqft_living15  21613 non-null  int64  \n",
      " 20  sqft_lot15     21613 non-null  int64  \n",
      "dtypes: float64(5), int64(15), object(1)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are few issues potentially: \n",
    "    \n",
    "    1. It's true that pretty much all our data is numeric, but looking back at the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing outliers, the 33 bedrooms/ bathrooms... that kind of thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection - Looking at what features contribute most"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation with Target Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplifying the dataset means removing columns that might not be relevant for our current analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are trying to investigate factors that affect the price and value of a home: \n",
    "\n",
    "1. So, \"id\" and \"date\" are irrelevant because they aren't really features pertaining to the house, not components of the house.\n",
    "REMOVED \"id\" and \"date\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that leaves us with 18 other features (18 other columns excluding \"price) to account for in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we can look at correlation between pairs of features to try to get an idea from there, what features may the most helpful in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So even with correlation values with each pair of variables, it's still a lot of information to digest.\n",
    "What are we looking for? : \n",
    "High absolute values in the \"price\" row/column\n",
    "\n",
    "To make it easier for us to discern, let's use a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df1.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a bit cluttered. (The lighter colors along the \"price\" row/column, represent the positive correlations between price and another variable).\n",
    "\n",
    "Isolating the \"price\" column can give us a better visual to draw conclusions from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_corrs = df1.corr()[\"price\"].map(abs).sort_values(ascending=False)\n",
    "df1_corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that there are \"price\" and the following variables have the least correlation:\n",
    "\n",
    "id, (date isn't on here but again irrelevant to house price), longitude, zipcode, yr_built, sqft_lot15, sqft_lot, yr_renovated.\n",
    "\n",
    "\n",
    "Those features listed above were all displayed in the latter end of the spectrum, mostly in purple, representing the least correlation with \"price\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The highest correlation was \"sqft_living\" (sqft footage of living space of home) to \"price\"; and \"grade\" (quality of the house) to \"price\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's look at those two features in this simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df1_preds = df1[[\"sqft_living\", \"grade\"]]\n",
    "df1_target = df1[\"price\"]\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(df1_preds, df1_target)\n",
    "lr.score(df1_preds, df1_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know about 53.5% of the variance surrounding data relating to \"price\"; We only know about 53.5% of what goes into calculating \"price\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "###### Now... there is an issue: How do we know if the large correlation values we are seeing are due to solely that of one feature to \"price\" (our target)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, our two features: \"sqft_living\" and \"grade\" both have high correlations to \"price\", 0.702 and 0.667, respectively.\n",
    "\n",
    "However, these two features have a high correlation toward each other: 0.7627.\n",
    "\n",
    "\n",
    "Because, we are specifically trying to investigate the features that contribute the most to price, we need to identify the effect (as much as possible) of the individual feature on our target, the \"price\" of the home.\n",
    "\n",
    "Therefore, multicollinearity between features is sort of a big deal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Our top contending features to use: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sqft_living      0.702035\n",
    "* grade            0.667434\n",
    "* sqft_above       0.605567\n",
    "* sqft_living15    0.585379\n",
    "* bathrooms        0.525138\n",
    "* view             0.397293\n",
    "* sqft_basement    0.323816\n",
    "* bedrooms         0.308350\n",
    "* lat              0.307003\n",
    "* waterfront       0.266369\n",
    "* floors           0.256794\n",
    "\n",
    "Those features above 0.20 correlation value are somewhat of interest. Those above 0.40 are of great interest as features for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df1[[\"sqft_lot\", \"sqft_living\",\n",
    "                    \"grade\", \"condition\", \"bathrooms\", \"bedrooms\",\n",
    "                    \"waterfront\", \"price\", \"floors\", \"lat\", \"long\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are eliminating the columns below:\n",
    "\n",
    "\n",
    "yr_built\n",
    "\n",
    "date\n",
    "\n",
    "view\n",
    "\n",
    "sqft_above\n",
    "\n",
    "sqft_basement\n",
    "\n",
    "yr_renovated\n",
    "\n",
    "zipcode\n",
    "\n",
    "lat\n",
    "\n",
    "long\n",
    "\n",
    "sqft_living15\n",
    "\n",
    "sqft_lot15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 21 to 11 columns to account for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to get a sense of the data, the values, for each feature and remove the outliers in preparation to building a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, that we want to change the datatypes for some the columns, for example \"price\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"price\"] = df[\"price\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may need to change the other features into another datatype. For now, this will do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prices Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependent variable here is price of the homes. Let's get a sense of the prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.price.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count    21,600\n",
    "\n",
    "mean     540,000\n",
    "\n",
    "std      367,000\n",
    "\n",
    "min      75,000\n",
    "\n",
    "25%      321,900\n",
    "\n",
    "50%      450,000\n",
    "\n",
    "75%      645000\n",
    "\n",
    "max      7,700,000\n",
    "\n",
    "(USD) 2014-2015\n",
    "King County, Washington 98001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easier to see now the corresponding numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.histplot(df.price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.thoughtco.com/what-is-the-interquartile-range-rule-3126244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q3, q1 = np.percentile(df[\"price\"], [75 ,25])\n",
    "iqr = q3 - q1\n",
    "iqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "323050 is the interquartile range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, ok- the 75percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the 25percentile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "323050*1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number will allow us to find the range that are outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though it's not often affected much by them, the interquartile range can be used to detect outliers. This is done using these steps:\n",
    "\n",
    "Calculate the interquartile range for the data.\n",
    "\n",
    "Multiply the interquartile range (IQR) by 1.5 (a constant used to discern outliers).\n",
    "\n",
    "Add 1.5 x (IQR) to the third quartile. Any number greater than this is a suspected outlier.\n",
    "\n",
    "Subtract 1.5 x (IQR) from the first quartile. Any number less than this is a suspected outlier.\n",
    "\n",
    "\n",
    "https://www.thoughtco.com/what-is-the-interquartile-range-rule-3126244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "645000+484575.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "321950-484575.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So regarding \"price\" values, any home price < -162625 and > 1129575 are outliers.\n",
    "And since we don't deal with negative numbers with price, we'll ignore the < -162625 part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_price_unique_values = df[\"price\"].unique()\n",
    "print(sorted(df_price_unique_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, ignoring the negative range because our prices start at 75,000. so let's drop values greater than 1129575."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, let's double check on how many entries we will be discarding before we do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "price_counts = df.groupby(\"price\")[\"price\"].agg(\"count\").sort_values(ascending=True)\n",
    "price_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_counts = df.groupby(\"price\")[\"price\"].agg(\"count\").sort_values(ascending=False)\n",
    "price_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since price values greater than 1129575 are outliers, we have to keep values less than or equal to 1129575."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_outliers_rmvd = df[df[\"price\"] <= 1129575]\n",
    "df_outliers_rmvd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.histplot(df_outliers_rmvd.price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new visual plot. Not the best, but with the outliers removed, it'll work for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_outliers_rmvd.price.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, trying to simplify the code: This will be out reuseable template for the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q3, q1 = np.percentile(df_outliers_rmvd[\"price\"], [75 ,25])\n",
    "iqr = q3 - q1\n",
    "print(\"iqr=\", iqr)\n",
    "print(\"q3=\", q3)\n",
    "print(\"q1=\", q1)\n",
    "print(\"constant=\", iqr*1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"suspected outliers are greater than this number:\", q3+(iqr*1.5))\n",
    "print(\"suspected outliers are less than this number\", q1-(iqr*1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>So regarding \"price\", any price value < -162625 and > 1129575 are outliers.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to create a reuseable template. We'll try it with Living Space Square Footage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Living Space Square Footage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_outliers_rmvd.sqft_living.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual Plot: Initial Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_outliers_rmvd[\"sqft_living\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, trying to take out the outliers to hopefully normalize the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q3, q1 = np.percentile(df_outliers_rmvd[\"sqft_living\"], [75 ,25])\n",
    "iqr = q3 - q1\n",
    "print(\"iqr=\", iqr)\n",
    "print(\"q3=\", q3)\n",
    "print(\"q1=\", q1)\n",
    "print(\"constant=\", iqr*1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"suspected outliers are greater than this number:\", q3+(iqr*1.5))\n",
    "print(\"suspected outliers are less than this number\", q1-(iqr*1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So regarding \"sqft_living\", any sqft_living value < -146.5 and > 3977.5 are outliers. Again, any negative numbers, we can sort of ignore, unless negative values start appearing on our histogram plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers_rmvd = df_outliers_rmvd[df_outliers_rmvd[\"sqft_living\"] <= 3977.5]\n",
    "df_outliers_rmvd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the new histogram plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.histplot(df_outliers_rmvd[\"sqft_living\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a bit crude but we can work with that for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lot Square Footage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_outliers_rmvd.sqft_lot.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual Plot: Initial Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_outliers_rmvd[\"sqft_lot\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, trying to take out the outliers to hopefully normalize the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q3, q1 = np.percentile(df_outliers_rmvd[\"sqft_lot\"], [75 ,25])\n",
    "iqr = q3 - q1\n",
    "print(\"iqr=\", iqr)\n",
    "print(\"q3=\", q3)\n",
    "print(\"q1=\", q1)\n",
    "print(\"constant=\", iqr*1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"suspected outliers are greater than this number:\", q3+(iqr*1.5))\n",
    "print(\"suspected outliers are less than this number\", q1-(iqr*1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So regarding \"sqft_living\", any sqft_living value < -2800.0 and > 18000.0 are outliers. Again, any negative numbers, we can sort of ignore, unless negative values start appearing on our histogram plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers_rmvd = df_outliers_rmvd[df_outliers_rmvd[\"sqft_lot\"] <= 18000]\n",
    "df_outliers_rmvd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the new histogram plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.histplot(df_outliers_rmvd[\"sqft_lot\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a bit crude but we can work with that for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And since lot_sqftspace is a bit difficult to discern for a general correlation, we might just scratch the feature altogether towards the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bedrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_outliers_rmvd.bedrooms.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual Plot: Initial Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_outliers_rmvd[\"bedrooms\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, trying to take out the outliers to hopefully normalize the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q3, q1 = np.percentile(df_outliers_rmvd[\"bedrooms\"], [75 ,25])\n",
    "iqr = q3 - q1\n",
    "print(\"iqr=\", iqr)\n",
    "print(\"q3=\", q3)\n",
    "print(\"q1=\", q1)\n",
    "print(\"constant=\", iqr*1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"suspected outliers are greater than this number:\", q3+(iqr*1.5))\n",
    "print(\"suspected outliers are less than this number\", q1-(iqr*1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So regarding \"sqft_living\", any sqft_living value < 1.5 and > 5.5 are outliers. Again, any negative numbers, we can sort of ignore, unless negative values start appearing on our histogram plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_outliers_rmvd = df_outliers_rmvd[df_outliers_rmvd[\"bedrooms\"]<= 5.5]\n",
    "df_outliers_rmvd = df_outliers_rmvd[df_outliers_rmvd[\"bedrooms\"]>= 1.5]\n",
    "df_outliers_rmvd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to double check that both portions of the range were kept and not discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_outliers_rmvd.loc[df_outliers_rmvd[\"bedrooms\"] <= 5.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers_rmvd.loc[df_outliers_rmvd[\"bedrooms\"] >= 1.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the new histogram plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.histplot(df_outliers_rmvd[\"bedrooms\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a bit crude but we can work with that for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very crude correlation and normal distribution curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bathrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_outliers_rmvd.bathrooms.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual Plot: Initial Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_outliers_rmvd[\"bathrooms\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, trying to take out the outliers to hopefully normalize the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q3, q1 = np.percentile(df_outliers_rmvd[\"bathrooms\"], [75 ,25])\n",
    "iqr = q3 - q1\n",
    "print(\"iqr=\", iqr)\n",
    "print(\"q3=\", q3)\n",
    "print(\"q1=\", q1)\n",
    "print(\"constant=\", iqr*1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"suspected outliers are greater than this number:\", q3+(iqr*1.5))\n",
    "print(\"suspected outliers are less than this number\", q1-(iqr*1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So regarding \"sqft_living\", any sqft_living value < 0 and > 4 are outliers. Again, any negative numbers, we can sort of ignore, unless negative values start appearing on our histogram plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers_rmvd = df_outliers_rmvd[df_outliers_rmvd[\"bathrooms\"] <= 4]\n",
    "df_outliers_rmvd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the new histogram plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.histplot(df_outliers_rmvd[\"bathrooms\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a bit crude but we can work with that for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very crude correlation as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now grade is one of those that need not remove outliers because we just need to understand what grade homes is considered more expensive. So just a correlation will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_outliers_rmvd.grade.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual Plot: Initial Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_outliers_rmvd[\"grade\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit crude, but we will work that in with price later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_outliers_rmvd.condition.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual Plot: Initial Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_outliers_rmvd[\"condition\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a bit crude but we can work with that for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Floors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_outliers_rmvd.floors.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual Plot: Initial Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_outliers_rmvd[\"floors\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No real correlation yet til we match with price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = sns.scatterplot(x=df_outliers_rmvd[\"long\"], y=df_outliers_rmvd[\"lat\"], hue=df_outliers_rmvd[\"price\"], palette=\"plasma\",\n",
    "                     marker=\".\")\n",
    "ax.set( xlabel=\"Longitude\",\n",
    "        ylabel=\"Latitude\", \n",
    "        title=\"Price by Location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems there is a general area from 47.55 North latitude to 47.7 North latitude, where most of the most expensive properties are located."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waterfront"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_outliers_rmvd.waterfront.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.histplot(df_outliers_rmvd[\"waterfront\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our analysis, we will exclude waterfront as a feature because it doesn't show discernibility, that it would impact price. Perhaps, with the removal of outliers, has skewed the model towards homes without waterfronts and it would be interesting to see the effect a waterfront has on the price. My prior limited background knowledge agrees with the fact that a waterfront property would be more expensive than a similar property without one.\n",
    "\n",
    "But right now that is my speculation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Multicolinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_matrix = df_outliers_rmvd.corr()\n",
    "print(corr_matrix[\"price\"].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Living area and grade have the highest correlations with price. Latitude visually showed more promise as a feature with a high correlation to price of the home."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare a model and see where our features are at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_outliers_rmvd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_outliers_rmvd.drop(\"price\", 1)\n",
    "y = df_outliers_rmvd[\"price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictors = sm.add_constant(X_train)\n",
    "model_0 = sm.OLS(y_train , predictors).fit()\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-Squred value is decent - An R^2 of 1 indicates that the regression predictions perfectly fit the data.\n",
    "Near zero p-values indicated strong evidence that the null hypothesis be rejected.\n",
    "<b>High Condition number</b>... something to watch out for too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr= LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Use Linear Regression to make predictions for train and test data\n",
    "y_hat_train = lr.predict(X_train)\n",
    "y_hat_test = lr.predict(X_test)\n",
    "\n",
    "\n",
    "# Calculate Root Mean Square Error\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_hat_train))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_hat_test))\n",
    "\n",
    "# Calculate Mean Absolute Error\n",
    "test_mae = mean_absolute_error(y_test, y_hat_test)\n",
    "train_mae = mean_absolute_error(y_train, y_hat_train)\n",
    "\n",
    "print(f\"Train Root Mean Square Error: {train_rmse}\")\n",
    "print(f\"Test Root Mean Square Error: {test_rmse}\")\n",
    "\n",
    "print(f\"Train Mean Absolute Error: {train_mae}\")\n",
    "print(f\"Test Mean Absolute Error: {test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = sm.graphics.qqplot(model_0.resid, dist=stats.norm, line='45', fit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This residual plot is not all that good, room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of this model is to see if scaling helps in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_log = np.log(df_outliers_rmvd.price)\n",
    "price_log = pd.DataFrame(price_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df_outliers_rmvd.drop('price', 1)\n",
    "y1 =price_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scalerp = StandardScaler()\n",
    "\n",
    "X_train1[[\"sqft_lot\", \"sqft_living\", \"bathrooms\", \"bedrooms\", \"floors\", \"grade\", \"lat\", \"long\"]]  =scaler.fit_transform(X_train1[[\"sqft_lot\", \"sqft_living\", \"bathrooms\", \"bedrooms\", \"floors\", \"grade\", \"lat\", \"long\"]])\n",
    "\n",
    "\n",
    "X_test1[[\"sqft_lot\", \"sqft_living\", \"bathrooms\", \"bedrooms\", \"floors\", \"grade\", \"lat\", \"long\"]] = scaler.transform(X_test1[[\"sqft_lot\", \"sqft_living\", \"bathrooms\", \"bedrooms\", \"floors\", \"grade\", \"lat\", \"long\"]])\n",
    "\n",
    "\n",
    "y_train1 = scalerp.fit_transform(pd.DataFrame(y_train1))\n",
    "y_test1 = scalerp.transform(pd.DataFrame(y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = sm.add_constant(X_train1)\n",
    "model_1 = sm.OLS(y_train1 , predictors).fit()\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue with the condition number is gone. And r-squared has jumped from 63% to 67%.\n",
    "So, that's sort of the good news. \n",
    "\n",
    "The bad news: the r-squared is still too low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr1= LinearRegression()\n",
    "lr1.fit(X_train1, y_train1)\n",
    "\n",
    "\n",
    "# Use Linear Regression to make predictions for train and test data\n",
    "y_hat_train = lr1.predict(X_train1)\n",
    "y_hat_test = lr1.predict(X_test1)\n",
    "\n",
    "\n",
    "\n",
    "# Undo scale\n",
    "y_train1 = scalerp.inverse_transform(y_train1)\n",
    "y_test1 = scalerp.inverse_transform(y_test1)\n",
    "y_hat_train = scalerp.inverse_transform(y_hat_train)\n",
    "y_hat_test = scalerp.inverse_transform(y_hat_test)\n",
    "\n",
    "# Undo log\n",
    "y_train1 = np.exp(y_train1)\n",
    "y_test1 = np.exp(y_test1)\n",
    "y_hat_train = np.exp(y_hat_train)\n",
    "y_hat_test = np.exp(y_hat_test)\n",
    "\n",
    "\n",
    "# Calculate Root Mean Square Error\n",
    "train_rmse1 = np.sqrt(mean_squared_error(y_train1, y_hat_train))\n",
    "test_rmse1 = np.sqrt(mean_squared_error(y_test1, y_hat_test))\n",
    "\n",
    "# Calculate Mean Absolute Error\n",
    "test_mae1 = mean_absolute_error(y_test1, y_hat_test)\n",
    "train_mae1 = mean_absolute_error(y_train1, y_hat_train)\n",
    "\n",
    "print(f'Train Root Mean Square Error: {train_rmse1}')\n",
    "print(f'Test Root Mean Square Error: {test_rmse1}')\n",
    "\n",
    "print(f'Train Mean Absolute Error: {train_mae1}')\n",
    "print(f'Test Mean Absolute Error: {test_mae1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_hat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = sm.graphics.qqplot(model_1.resid, dist=stats.norm, line='45', fit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here's the dilemma: we don't want a model to be too fitted, overfitted, because then it really isn't any use as a model to predict. It's nothing more than a glorified calculator that spit out calculations and numbers for existing data. \n",
    "\n",
    "However, we want it to have some degree of fit to the line so that it CAN be used as a model.\n",
    "\n",
    "A happy medium somewhere in there..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = [ ['Model 0', train_rmse, test_rmse, train_mae, test_mae],\n",
    "            ['Model 1',train_rmse1, test_rmse1, train_mae1, test_mae1]]\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=['Model', 'Train RMSE', 'Test RMSE', 'Train MAE', 'Test MAE'])\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRITICAL-Model Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think I'll go with model 2 because the scaling brought down the condition number, visually it was more aesthetically pleasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically lower RSME shows better fit to the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xf = df_outliers_rmvd.drop('price', 1)\n",
    "\n",
    "scalerf= StandardScaler()\n",
    "\n",
    "Xf[[\"sqft_lot\", \"sqft_living\", \"bathrooms\", \"bedrooms\", \"floors\", \"grade\", \"condition\", \"lat\", \"long\"]]  =scalerf.fit_transform(Xf[[\"sqft_lot\", \"sqft_living\", \"bathrooms\", \"bedrooms\", \"floors\", \"grade\", \"condition\", \"lat\", \"long\"]])\n",
    "\n",
    "scalerfp = StandardScaler()\n",
    "\n",
    "price_sc = scalerp.transform(pd.DataFrame(df_outliers_rmvd.price))\n",
    "\n",
    "y_hat = lr1.predict(Xf)\n",
    "\n",
    "y_hat = np.exp(scalerp.inverse_transform(y_hat))\n",
    "\n",
    "y_hat\n",
    "\n",
    "rmse_f = np.sqrt(mean_squared_error(df_outliers_rmvd.price , y_hat))\n",
    "mae_f = mean_absolute_error(df_outliers_rmvd.price, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Root Mean Square Error: {rmse_f}')\n",
    "print(f'Mean Absolute Error: {mae_f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mae_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mae_f/df_outliers_rmvd.price.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.histplot(model_1.resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = sm.graphics.qqplot(model_1.resid, dist=stats.norm, line='45', fit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.regplot(x=np.log(y_test1), y=np.log(y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the plot conforms to the best fit line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Question - Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The factors most affecting the price of a house are:\n",
    "\n",
    "* Location(lat)\n",
    "\n",
    "* Quality of the house(grade)\n",
    "\n",
    "* Living area(sqft_living)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have a model that has an Coefficient of Determination(R-squared) value of 0.672 which indicates that our model can explain 67.2% of all variation in the data around the mean.\n",
    "\n",
    "* With a Mean Squared Error of around 140227 USD, that means our predicted price is, on average, 140227 USD off from our mean. While that number doesn't look too bad our Root Mean Squared Error is around 183833 USD which means that our model is being heavily penalized for predictions that are very far off the actual price.\n",
    "\n",
    "* Average home price: 476,985 USD. The price prediction was +/-$140,227 off the real price (29.4% margin of error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive analysis and modeling reveal which factors contribute most to housing prices: \n",
    "\n",
    "● Increase Living Area(in square feet) \n",
    "\n",
    "● Buy homes in regions specified (47.55 15°N to 47.7 15°N) (Or \n",
    "    maybe homes outside of this region will likely be more affordable) \n",
    "    \n",
    "● Upgrade the quality of your home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The data we were provided was from 2014 to 2015. And such outdated data may not give us the optimal insights relevant to \n",
    "  today's housing situation\n",
    "\n",
    "* We should be able to get a lot more out of the location data, with further analysis, incorporating data relevant to the \n",
    "  zipcode so there is a better determination for prices that can be expected in a more defined area.\n",
    "\n",
    "* Also, streamlining the methods of getting a more fitted model without going too far into \"overfitted\" territory. \n",
    "  Like I've mentioned before, there is a happy medium in there.\n",
    "\n",
    "* The most obvious next step is to try out new modeling techniques.  While linear regression is a good start, there are many \n",
    "  other techniques that I believe could help make better predictions.  Of particular interest to me in this context are \n",
    "  Polynomial Regression and Weighted Least Squares, that might be promising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11,8))\n",
    "ax = sns.regplot(data=df_outliers_rmvd, x=\"sqft_living\", y=\"price\", marker=\".\",\n",
    "     scatter_kws={\"color\": \"grey\"}, line_kws={\"color\": \"blue\"})\n",
    "\n",
    "ax.set(  xlabel=\"Living Area(square feet)\",\n",
    "         ylabel=\"Price(in Millions of $)\", \n",
    "         title=\"Price by Living Area\",\n",
    " )\n",
    "\n",
    "\n",
    "plt.xlim([0,5000])\n",
    "plt.ylim([0, 1250000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_outliers_rmvd.grade.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,20))\n",
    "ax = sns.barplot(data=df_outliers_rmvd, x=\"grade\", y=\"price\", ci=None)\n",
    "ax.set( xticklabels=([\"4\",\"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\"]),\n",
    "        xlabel=\"Grade\",\n",
    "        ylabel=\"Price (in Thousands of $)\", \n",
    "        title=\"Price by Grade\"  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
